?findInterval
colWeightedMeans
colWeightedMeans
vcoc
vcov
?vcoc
?vcov
vcovCL
vcov
substitute()
substitute()
substitute
?substitute
deparse()
?deparse
x = runig(500)
x = runif(500)
x
substitute(x)
xs = substitute(x)
xs
class(xs)
x
x
substiture(s)
substiture(x)
substitute(x)
deparse(substitute(x))
x
x1 = "something"
x1
substitute(x)
substitute(x1)
xname <- deparse(substitute(x))
yname <- deparse(substitute(y))
byname <- deparse(substitute(by))
weightsname <- deparse(substitute(weights))
subsetname  <- deparse(substitute(subset))
clustername <- deparse(substitute(cluster))
xname <- deparse(substitute(x))
yname <- deparse(substitute(y))
byname <- deparse(substitute(by))
weightsname <- deparse(substitute(weights))
subsetname  <- deparse(substitute(subset))
clustername <- deparse(substitute(cluster))
x
x = c(1,2,3,4)
xname <- deparse(substitute(x))
yname <- deparse(substitute(y))
byname <- deparse(substitute(by))
weightsname <- deparse(substitute(weights))
subsetname  <- deparse(substitute(subset))
clustername <- deparse(substitute(cluster))
xname
x
x = "ola"
deparse(substitute(x))
x
library(glmnet)
glmnet::glmnet()
? glmnet
n = 10
p  = 3
X = matrix(rnorm(n*p),n,p)
X
beta = c(3, rep (0,p-1))
beta
U = rnorm(n)
U
Y = X %*%beta + U
Y
X
t(X)
Hi Norman,
Thanks for the quick response. Let me check the dates, and I will get back to you soon.
I prefer to present in person, mainly cause I live so close, and I guess we are all tired of Zoom meetings/lectures. However, if Covid does not allow us, we can do it remotely.
Best
Ricardo
t(X)%*% X
solve(t(X)%*% X)
solve(t(X)%*% X) %*% t(X)%*% X
# Regress Y em X
beta_hat = solve(t(X)%*% X) %*%  X %*% Y
# Regress Y em X
beta_hat = solve(t(X)%*% X) %*% t(X) %*% Y
beta_hat
source('~/.active-rstudio-document')
beta_hat
n = 10
p  = 9
X = matrix(rnorm(n*p),n,p)
beta = c(3, rep (0,p-1))
U = rnorm(n)
Y = X %*%beta + U
# Regress Y em X
beta_hat = solve(t(X)%*% X) %*% t(X) %*% Y
beta_hat
beta
n = 10
p  = 10
X = matrix(rnorm(n*p),n,p)
beta = c(3, rep (0,p-1))
U = rnorm(n)
Y = X %*%beta + U
# Regress Y em X
beta_hat = solve(t(X)%*% X) %*% t(X) %*% Y
beta_hat
n = 10
p  = 11
X = matrix(rnorm(n*p),n,p)
beta = c(3, rep (0,p-1))
U = rnorm(n)
Y = X %*%beta + U
# Regress Y em X
beta_hat = solve(t(X)%*% X) %*% t(X) %*% Y
1/0
n = 10
p  = 50
X = matrix(rnorm(n*p),n,p)
beta = c(3, rep (0,p-1))
U = rnorm(n)
Y = X %*%beta + U
# Regress Y em X
beta_hat = solve(t(X)%*% X) %*% t(X) %*% Y
# Ridge Regression
install.packages("glmnet")
install.packages("glmnet")
library(glmnet)
glmnet()
?glmnety
?glmnet
# Run a Ridge regression
results = glmnet(x,y,alpha = 0)
n = 100
p  = 50
X = matrix(rnorm(n*p),n,p)
beta = c(3, rep (0,p-1))
U = rnorm(n)
Y = X %*%beta + U
# Run a Ridge regression
results = glmnet(X,Y,alpha = 0)
resutls
resutts
results
# Display the estimated coefficient
coef(results)
n = 100
p  = 10
X = matrix(rnorm(n*p),n,p)
beta = c(3, rep (0,p-1))
U = rnorm(n)
Y = X %*%beta + U
results = glmnet(X,Y,alpha = 0)
# Display the estimated coefficient
coef(results)
# Display the estimated coefficient
beta_ridge = coef(results)
results = glmnet(X,Y,alpha = 0, lambda = 1)
# Display the estimated coefficient
beta_ridge = coef(results)
beta_ridge
results
results = glmnet(X,Y,alpha = 0)
results
y_hat = predict(resutls)
y_hat = predict(results)
y_hat = predict(results, newx = X)
y_hat
Xnew = rnorm(p)
Xnew
plot(results)
plot(coef(resutls))
plot(coef(results))
1:100
lambda_grid = 1:100
lambda_grid
results = glmnet(X,Y,alpha = 0, lambda = lambda_grid)
results
?cv.glmnet
result = cv.glmnet(x,y, nfolds = 5)
result = cv.glmnet(X,Y, nfolds = 5)
result
result$lambda.min
lambda_min = result$lambda.min
lambda_min
result_lambda_min = glmnet(X,Y,lambda = lambda_min)
beta_ridge_cv = coef(result_lambda_min)
beta_ridge_cv
result = cv.glmnet(X,Y, nfolds = 5, alpha =0)
lambda_min = result$lambda.min
result_lambda_min = glmnet(X,Y,lambda = lambda_min, alpha = 0)
beta_ridge_cv = coef(result_lambda_min)
beta_ridge_cv
n = 10
p  = 1000
X = matrix(rnorm(n*p),n,p)
beta = c(3, rep (0,p-1))
U = rnorm(n)
Y = X %*%beta + U
result = cv.glmnet(X,Y, nfolds = 5, alpha =0)
lambda_min = result$lambda.min
result_lambda_min = glmnet(X,Y,lambda = lambda_min, alpha = 0)
beta_ridge_cv = coef(result_lambda_min)
beta_ridge_cv
# Generate Data
n = 100
p  = 1000
X = matrix(rnorm(n*p),n,p)
beta = c(3, rep (0,p-1))
U = rnorm(n)
Y = X %*%beta + U
dim(X)
lenght(Y)
length(Y)
# OLS
beta_hat = solve(t(X)%*% X) %*% t(X) %*% Y
library(glmnet)
?glmnet
result = glmnet(X,Y,alpha = 1)
result
result$beta
plot(results)
plot(result)
result = cv.glmnet(X,Y, nfolds = 5, alpha =1)
plot(result)
plot(result)
result$lambda.min
lambda_best
lambda_best = result$lambda.min
lambda_best
result_best = glmnet(X,Y,alpha = 1, lambda = lambda_best)
result_best$beta
result$nzero
result$beta
result$index
result_best$a0
result_best
predict(result_lambda_min, nexw = x)
predict(result_best, nexw = x)
predict(result_best, nexw = X)
predict(result_best)
predict(result_best, newx = X)
x_novo = rnorm(100)
x_novo
y_hat_oos = predict(result_best, newx = x_novo)
x_novo = rnorm(1000)
x_novo
y_hat_oos = predict(result_best, newx = x_novo)
y_hat_oos
summary(result_best)
x_novo = matrix(rnorm(n_oos*p),n_oss,p)
n_oos = 30
x_novo = matrix(rnorm(n_oos*p),n_oss,p)
n_oos = 30
x_novo = matrix(rnorm(n_oos*p),n_oos,p)
dim(x)
dim(x_novo)
y_novo_oss = predict(result_best, newx = x_novo)
y_hat_oss
y_hat_oos
x_novo = matrix(rnorm(n_oos*p),n_oos,p)
x_novo
dim(x_novo)
predict(result_best, newx = x_novo)
y_novo = x_novo %*%beta + rnorm(n_oss)
y_novo = x_novo %*%beta + rnorm(n_oos)
x_nov
x_novo
dim(x_novo)
length(y_novo)
y_novo_hat = predict(result_best, newx = x_novo)
dim(y_novo_hat)
var(y_novo)
res = y_novo - y_novo_hat
res
var(res)
num =
R2_oos = 1 - var(res)/var(y_novo)
R2_oss
R2_oos
cor(x)
cor(X)
MC = cor(X)
dim(MC)
MC(34,734)
MC[34,734]
result_best$beta
MC[1,10]
range(rnorm(20))
tete = sapply(1:10, function(x) range(rnorm(20)))
tete
seq(np.max(xminmat), np.min(xmaxmat), length=simsgrid+2)
seq(max(xminmat), min(xmaxmat), length=simsgrid+2)
seq(max(xminmat), min(xmaxmat), length=5)
seq(max(xminmat), min(xmaxmat), length=15)
seq(max(rnorm(20)), min(rnorm(20)), length=15)
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
source('~/Dropbox/binsreg/R version/binsreg/test/test.R')
y = rnorm(20)
y
lm(y ~ 1)
# y = beta_0*1 + error
lm(y ~ 1)
# beta_0_chapeu = 0.1307
mean(y)
x = rnorm(20)
u = rnorm(20)
y  = 3*x + u
# y = beta_0*1 + error
mod1 = lm(y ~ x)
summary(mod1)
u_hat = mod1$residuals
x = rnorm(20)
u = rnorm(20)
y  = 3*x + u
# Stage 0
mod0 = lm(y ~ x)
summary(mod0)
mean(y)
u_0 = mod0$residuals
# Stage 1
mod1 = lm(u_0 ~ x)
summary(mod1)
# Stage 1
mod1 = lm(u_0 ~ x^2)
summary(mod1)
mean(y)
u_2 = mod1$residuals
summary(mod1)
# Stage 0
mod0 = lm(y ~ x)
summary(mod0)
u_0 = mod0$residuals
# Stage 1
mod1 = lm(u_0 ~ x^2)
summary(mod1)
u_2 = mod1$residuals
# Stage 0
mod0 = lm(y ~ x)
summary(mod0)
u_0 = mod0$residuals
# Stage 1
mod1 = lm(u_0 ~ x)
summary(mod1)
u_2 = mod1$residuals
# Bagged Mean
n = 100
x = 3 + rnorm(100)
x
help(sample)
x_star = sample(x, size = n, replace = TRUE)
x_star
n = 100
B = 10000
x = 3 + rnorm(100)
x_star = sample(x, size = n, replace = TRUE)
mean_star = mean(x_star)
mean_star
n = 100
B = 10000
x = 3 + rnorm(100)
x_star = sample(x, size = n, replace = TRUE)
mean_star = mean(x_star)
n = 100
B = 10000
x = 3 + rnorm(100)
x_star = sample(x, size = n, replace = TRUE)
mean_star
mean_boot = rep(NA,B)
mean_boot
n = 100
B = 10000
x = 3 + rnorm(100)
mean_boot = rep(NA,B)
for j in 1:B {
x_star = sample(x, size = n, replace = TRUE)
mean_boot[j] = mean(x_star)
}
n = 100
B = 10000
x = 3 + rnorm(100)
mean_boot = rep(NA,B)
for (j in 1:B) {
x_star = sample(x, size = n, replace = TRUE)
mean_boot[j] = mean(x_star)
}
mean_boot
hist(mean_boot)
n = 100
B = 10000
x = 3 + rnorm(n)
mean_boot = rep(NA,B)
for (j in 1:B) {
x_star = sample(x, size = n, replace = TRUE)
mean_boot[j] = mean(x_star)
}
hist(mean_boot)
mean_final = mean(mean_boot)
mean_final
mean(x)
iris
help(randomForest)
# Calculate the size of each of the data sets:
data_set_size <- floor(nrow(iris)/2)
help(randomForest)
help(randomForest)
library(randomForest)
help(randomForest)
iris
# Perform training:
rf_classifier = randomForest(Species ~ ., data=training, ntree=100, mtry=2, importance=TRUE)
# Calculate the size of each of the data sets:
data_set_size <- floor(nrow(iris)/2)
data_set_size
1:nrow(iris)
sample(1:nrow(iris), size = data_set_size)
set.seed(17)
# Calculate the size of each of the data sets:
data_set_size <- floor(nrow(iris)/2)
# Generate a random sample of "data_set_size" indexes
indexes <- sample(1:nrow(iris), size = data_set_size)
# Assign the data to the correct sets
training <- iris[indexes,]
validation1 <- iris[-indexes,]
irtis
iris
training
validation1
# Perform training:
rf_classifier = randomForest(Species ~ ., data=training, ntree=100, mtry=2, importance=TRUE)
rf_classifier
rf_test = predict(rf_classifier, newdata = testing)
rf_test = predict(rf_classifier, newdata = validation1)
ref_test
rf_test
varImpPlot(rf_classifier)
rf_test
rf_test==setosa
rf_test=='setosa'
mean(rf_test=='setosa')
mean(rf_test=='versicolor')
mean(rf_test=='virginica')
validation1
y = Validation1$Species
y = validation1$Species
y
y == rf_test
mean(y == rf_test)
rf_classifier
mean(y[y=='setosa'] == rf_test[y=='setosa'])
mean(y[y=='versicolor'] == rf_test[y=='versicolor'])
mean(y[y=='virginica'] == rf_test[y=='virginica'])
data_set_size <- floor(nrow(iris)/2)
# Generate a random sample of "data_set_size" indexes
indexes <- sample(1:nrow(iris), size = data_set_size)
# Assign the data to the correct sets
training <- iris[indexes,]
validation1 <- iris[-indexes,]
# We will use the "randomForest" package
#install.packages("randomForest")
#library(randomForest)
#help(randomForest)
# Perform training:
rf_classifier = randomForest(Species ~ ., data=training, ntree=100, mtry=2, importance=TRUE)
rf_test = predict(rf_classifier, newdata = validation1)
# Acuracidade OOS
y = Validation1$Species
mean(y == rf_test)
mean(y[y=='setosa'] == rf_test[y=='setosa'])
mean(y[y=='versicolor'] == rf_test[y=='versicolor'])
mean(y[y=='virginica'] == rf_test[y=='virginica'])
rf_classifier
# Calculate the size of each of the data sets:
data_set_size <- floor(nrow(iris)/2)
# Generate a random sample of "data_set_size" indexes
indexes <- sample(1:nrow(iris), size = data_set_size)
# Assign the data to the correct sets
training <- iris[indexes,]
validation1 <- iris[-indexes,]
# We will use the "randomForest" package
#install.packages("randomForest")
#library(randomForest)
#help(randomForest)
# Perform training:
rf_classifier = randomForest(Species ~ ., data=training, ntree=100, mtry=2, importance=TRUE)
rf_test = predict(rf_classifier, newdata = validation1)
# Acuracidade OOS
y = Validation1$Species
rf_classifier
mean(y == rf_test)
mean(y[y=='setosa'] == rf_test[y=='setosa'])
mean(y[y=='versicolor'] == rf_test[y=='versicolor'])
mean(y[y=='virginica'] == rf_test[y=='virginica'])
